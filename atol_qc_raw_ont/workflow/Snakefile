#!/usr/bin/env python3

import tempfile
from pathlib import PosixPath


def get_porechop_results(wildcards):
    all_names = get_readfile_names(wildcards)
    porechop_results = expand(
        Path(workingdir, "porechop", "{read_file}.fastq"), read_file=all_names
    )
    return porechop_results


def map_read_file_to_name(reads):
    if not isinstance(reads, list):
        raise ValueError("Input must be a list of read files.")

    read_file_to_name = {}

    for file in reads:
        if not isinstance(file, PosixPath):
            raise ValueError(f"File {file} is not a Path() object")

        name = file.name.split(".")[0]

        if name in read_file_to_name:
            logger.error(f"Read files:     {reads}")
            logger.error(f"Existing names: {sorted(set(read_file_to_name.keys()))}")
            logger.error(f"Duplicate name: {name}")
            raise ValueError(f"The name of readfile {file} is not unique.")

        read_file_to_name[name] = file

    return read_file_to_name


globals().update(config)
# workingdir = tempfile.mkdtemp()
workingdir = Path("test-output", "tmp")

logger.warning(f"Using {workingdir} for intermediate files")

if not logs_directory:
    logger.warning(f"Not keeping logs")
    logs_directory = workingdir
else:
    logger.warning(f"Saving logs to {logs_directory}")


if reads_tarfile:
    include: "rules/single_tarfile.smk"

elif reads:
    read_file_to_name = map_read_file_to_name(reads)

    def get_readfile_names(wildcards):
        all_names = sorted(set(read_file_to_name.keys()))

        wildcard_constraints:
            read_file="|".join(all_names)

        return all_names

    def get_readfile(wildcards):
        return read_file_to_name[wildcards.read_file]

else:
    raise ValueError("How did I get here?")



# TODO: use reformat.sh here to collect output stats
rule compress_output:
    input:
        Path(workingdir, "filtlong.fastq"),
    output:
        reads_out,
    log:
        Path(logs_directory, "compress_output.log"),
    threads: workflow.cores - 1
    shell:
        "cat {input} "
        "| "
        "pigz -p {threads} "
        "-9 "
        "> {output} "
        "2> {log}"


rule filtlong:
    input:
        Path(workingdir, "porechop.fastq"),
    output:
        pipe(Path(workingdir, "filtlong.fastq")),
    params:
        min_length=min_length,
    log:
        Path(logs_directory, "filtlong.log"),
    shell:
        "filtlong "
        "--min_length {params.min_length} "
        "{input} "
        ">> {output} "
        "2> {log}"


# filtlong reads the input file twice, so you have to write it to disk :(
rule collect_porechop_results:
    input:
        get_porechop_results,
    output:
        temp(Path(workingdir, "porechop.fastq")),
    shell:
        "cat {input} > {output}"


rule porechop:
    input:
        Path(workingdir, "collect_reads", "{read_file}.fastq"),
    output:
        temp(Path(workingdir, "porechop", "{read_file}.fastq")),
    log:
        Path(logs_directory, "porechop", "{read_file}.log"),
    threads: 1
    shell:
        "porechop "
        "-i {input} "
        "-o {output} "
        "--verbosity 1 "
        "--threads {threads} "
        "--discard_middle "
        "&> {log}"


# TODO: use reformat.sh here to collect input stats
rule collect_reads:
    input:
        get_readfile,
    output:
        temp(Path(workingdir, "collect_reads", "{read_file}.fastq")),
    log:
        Path(logs_directory, "collect_reads", "{read_file}.log"),
    threads: 1
    shell:
        "pigz -p 1 -d "
        "< {input} "
        "> {output} "
        "2>{log}"


rule target:
    default_target: True
    input:
        reads_out,
        # rules.expand_tarfile.output,
